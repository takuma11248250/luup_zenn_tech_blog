---
title: "Airflowã§ä¾å­˜é–¢ä¿‚ã®ä¿®æ­£æ¼ã‚Œã‚’é˜²ã"
emoji: "ğŸŒˆ"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢è¨˜äº‹ã€‚ã©ã¡ã‚‰ã§ã‚‚OK
topics: ["Airflow", "CI", "ExternalTaskSensor", "DataEngineering"]
publication_name: "luup_developers"
published: true # å…¬é–‹è¨­å®šï¼ˆtrueã«ã—ã¦mainãƒ–ãƒ©ãƒ³ãƒã«ãƒãƒ¼ã‚¸ã™ã‚‹ã¨ä¸€èˆ¬å…¬é–‹é–‹å§‹ï¼‰
---

ã“ã‚“ã«ã¡ã¯ã€‚Data Engineeringãƒãƒ¼ãƒ ã®æ²³é‡([@matako1124](https://twitter.com/matako1124)) ã§ã™ï¼

Airflowã§ä¾å­˜é–¢ä¿‚ã®è¨­å®šã«ExternalTaskSensorã‚’ä½¿ã£ã¦ã„ã‚‹ã®ã§ã™ãŒã€ExternalTaskSensorã‚’ä½¿ç”¨ã™ã‚‹æ™‚ã¯ã€ä»¥ä¸‹2ã¤ã‚’æ³¨æ„ã—ãªã‘ã‚Œã°ã„ã‘ã¾ã›ã‚“ã€‚

1. å®Ÿè¡Œæ™‚é–“ãŒåŒã˜ã§ãªã‘ã‚Œã°ã„ã‘ãªã„
2. DAGåã€Taskåã‚‚æ­£ç¢ºã«è¨˜è¼‰ã—ãªã„ã¨ã„ã‘ãªã„

ã“ã‚Œã‚’é–“é•ãˆã¦ã—ã¾ã†ã¨ã€Pokingã—ç¶šã‘ã¦ã—ã¾ã„ã¾ã™ã€‚

ç‰¹ã«ãƒªãƒ•ã‚¡ã‚¯ã‚¿ã§DAGåã‚„TaskåãŒå¤‰ã‚ã‚‹ã¨ã„ã£ãŸã“ã¨ã¯ã‚ˆãèµ·ã“ã‚Šã†ã‚‹ã¨æ€ã„ã¾ã™ã€‚
ä»Šå›ã¯ã€ŒDAGåã€TaskåãŒæ­£ç¢ºã«è¨˜è¼‰ã•ã‚Œã¦ã„ãªã‘ã‚Œã°ã€CIã§è½ã¨ã—ã€ä¿®æ­£æ¼ã‚Œã‚’é˜²ãã€äº‹ä¾‹ã‚’ã”ç´¹ä»‹ã—ã¾ã™ã€‚

## æ³¨æ„

- **åŸ·ç­†ã«å½“ãŸã‚Šç´°å¿ƒã®æ³¨æ„ã‚’æ‰•ã£ã¦ãŠã‚Šã¾ã™ãŒã€ä¸ååˆ†ãªèª¬æ˜ã‚„èª¤ã‚ŠãŒã‚ã‚‹å¯èƒ½æ€§ã‚‚ã”ã–ã„ã¾ã™ã€‚**
- **è¨˜äº‹å†…ã§ç´¹ä»‹ã—ã¦ã„ã‚‹ã‚³ãƒ¼ãƒ‰ã¯éƒ¨åˆ†çš„ãªã‚‚ã®ã§ã‚ã‚Šã€å‚è€ƒç¨‹åº¦ã«ã”å‚ç…§ãã ã•ã„ã€‚**

## ç›®æ¬¡

- ã©ã†ã„ã†æ™‚ã«å›°ã£ãŸã‹
- CIã¸ã®è¿½åŠ å®Ÿè£…äº‹ä¾‹
- çµ‚ã‚ã‚Šã«

## ã©ã†ã„ã†æ™‚ã«å›°ã£ãŸã‹

å®Ÿè£…ä¾‹ã‚’å–ã‚Šä¸Šã’ãªãŒã‚‰ã€ã©ã†ã„ã†æ™‚ã«å›°ã£ãŸã‹èª¬æ˜ã—ã¾ã™ã€‚
ç¾çŠ¶BigQueryã¸ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¯ã€1ã¤ã®wrapãƒ•ã‚¡ã‚¤ãƒ«ã«ã¾ã¨ã‚ã¦ãŠã‚Šã€ãã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—ã¦å‡¦ç†ã‚’è¡Œãªã£ã¦ã„ã¾ã™ã€‚
ä¾‹ãˆã°ã€ä»¥ä¸‹ã¯ä¸»ã«Masterãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã™ã‚‹éš›ã«å‘¼ã³å‡ºã—ã¦ã„ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã§ã€1ã€œ3ã¾ã§ã‚’TaskGroupã§1ã¤ã®Taskã«ã¾ã¨ã‚ã¦ã„ã¾ã™ã€‚

1. ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤
2. ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ–°è¦ä½œæˆ
3. ãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹

```python
def delete_create_load(
    self,
    dataset_id: str,
    table_name: str,
    table_description: str,
    partition_setting: dict = None,
) -> TaskGroup:

    schema_relative_path = "{}/src/schema".format(self.dags_folder)
    sql_relative_path = "/src/sql"
    table_resource = {
        "schema": {"fields": [json_file_load(schema_path, table_name)]},
        "description": table_description,
    }

    with TaskGroup(
        group_id="delete_create_load_table_{}".format(table_name)
    ) as delete_create_load_trans:
        delete_table = BigQueryDeleteTableOperator(
            task_id="delete_table",
            deletion_dataset_table="{}.{}.{}".format(
                self.project_id, dataset_id, table_name
            ),
            ignore_if_missing=True,
        )

        create_empty_table = BigQueryCreateEmptyTableOperator(
            task_id="create_empty_table",
            project_id=self.project_id,
            dataset_id=dataset_id,
            table_id=table_name,
            table_resource=table_resource,
        )

        load_table = BigQueryInsertJobOperator(
            task_id="load_table",
            configuration={
                "query": {
                    "query": f"{{% include '{'{}/{}.sql'.format(sql_path, table_name)}' %}}",
                    "useLegacySql": False,
                    "writeDisposition": "WRITE_EMPTY",
                    "destinationTable": {
                        "projectId": self.project_id,
                        "datasetId": dataset_id,
                        "tableId": table_name,
                    },
                }
            },
        )

    delete_table >> create_empty_table >> load_table
    return delete_create_load_trans
```

ä¸Šè¨˜ãƒ¡ã‚½ãƒƒãƒ‰ã‚’DAGã‹ã‚‰å‘¼ã³ã¾ã™ã€‚ã“ã‚Œã‚’DAG_Aã¨ã—ã¾ã™ã€‚

```python
from src.udf.wrap_bq_udf import WrapUnityBqUdf

wrap_unity_bq_udf = WrapUnityBqUdf(
    config_unity_dag.project_id,
    config_unity_dag.bucket_name,
    config_unity_dag.dags_folder,
)

with airflow.DAG(
    dag_id="DAG_A",
    default_args=config_unity_dag.config_default_args(
        owner, execution_timeout_minutes=120
    ),
    ...
    tags=["dwh"],
) as dag:

    cities_master = wrap_unity_bq_udf.delete_create_load(
        dataset_id=dataset_id,
        table_name="cities_master",
        table_description="Luupå®šç¾©ã‚·ãƒ†ã‚£ã®ãƒã‚¹ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«",
    )

```

DAG_Aã§ä½œæˆã—ãŸcities_masterã®Statusã‚’ExternalTaskSensorã§ã‚­ãƒ£ãƒƒãƒã—ã¾ã™ã€‚ã“ã‚Œã‚’DAG_Bã¨ã—ã¾ã™ã€‚

```python
from airflow.sensors.external_task import ExternalTaskSensor

with airflow.DAG(
    os.path.splitext(os.path.basename(__file__))[0],
    default_args=config_unity_dag.config_default_args(owner),
    ...
    tags=["datamart"],
) as dag:

    wait_for_dwh_load_cities_master = ExternalTaskSensor(
        task_id="wait_for_dwh_load_cities_master",
        external_dag_id="DAG_A",
        external_task_id="delete_create_load_table_cities_master.load_table",
    )
```

ã“ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã¦ã„ãŸå ´åˆã«ã€å¤§æœ¬ã®TaskGroupã®group_idã®å‘½åã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«å¤‰æ›´ã—ãŸã¨ã—ã¾ã™ã€‚

```python
group_id=table_name
```

â†“

```python
group_id="delete_create_load_table_{}".format(table_name)
```

group_idã‚’å¤‰ãˆã‚‹ã¨Taskã®å‘½åã‚‚å¤‰ã‚ã‚‹ã®ã§ã€ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³è¾¼ã‚“ã§ã„ã‚‹DAG_Aã ã‘ã§ãªãã€ExternalTaskSensorã§æŒ‡å®šã—ã¦ã„ãŸexternal_task_idã®å‘½å(DAG_B)ã‚‚ä¿®æ­£ã—ãªã„ã¨ã„ã‘ã¾ã›ã‚“ã€‚

ä¾‹ãˆã°ã€table_name="cities_master"ã ã¨ã™ã‚‹ã¨

```python
wait_for_dwh_load_cities_master = ExternalTaskSensor(
    ...
    external_task_id="cities_master.load_table",
)
```

ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«ä¿®æ­£ã—ãªã„ã¨ã€Pokingã—ç¶šã‘ã¦ã‚¨ãƒ©ãƒ¼ã§è½ã¡ã¦ã—ã¾ã„ã¾ã™ã€‚

```python
wait_for_dwh_load_cities_master = ExternalTaskSensor(
    ...
    external_task_id="delete_create_load_table_cities_master.load_table",
)
```

ä¸Šè¨˜ã®delete_create_loadãƒ¡ã‚½ãƒƒãƒ‰ã¯ã„ã‚ã‚“ãªDAGã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ã®ã§ã€æ°—è»½ã«ãƒªãƒ•ã‚¡ã‚¯ã‚¿ã§ããªããªã£ã¦ã—ã¾ã„ã¾ã—ãŸã€‚

## CIã¸ã®è¿½åŠ å®Ÿè£…äº‹ä¾‹

ãã“ã§ã€TaskSensorã«æŒ‡å®šã—ã¦ã„ã‚‹ExternalDagIdã€ExternalTaskIdãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹å‡¦ç†ã‚’CIã«çµ„ã¿è¾¼ã¿ã¾ã—ãŸã€‚
ä»¥ä¸‹ã®ã‚ˆã†ãªDagValidationã‚’CIã«çµ„ã¿è¾¼ã‚“ã§ã„ã¦ã€ã“ã“ã«ExternalDagIdã€ExternalTaskIdãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹å‡¦ç†ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚

```python
from airflow.sensors.external_task import ExternalTaskSensor

    def test_check_external_task_existence(self):
        ## TaskSensorã«æŒ‡å®šã—ã¦ã„ã‚‹ExternalDagIdã€ExternalTaskIdãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹
        external_task_sensor = [
            task
            for dag in self.dagbag.dags.values()
            for task in dag.tasks
            if isinstance(task, ExternalTaskSensor)
        ]
        for task in external_task_sensor:
            external_dag_id = task.external_dag_id
            external_task_id = task.external_task_id
            self.assertIn(
                external_dag_id, self.dagbag.dags, f"{external_dag_id} not found"
            )
            self.assertIn(
                external_task_id,
                self.dagbag.get_dag(external_dag_id).task_ids,
                f"{external_task_id} not found in {external_dag_id}",
            )
```

ã“ã‚Œã§DAGåã‚„Taskåã‚’å¤‰æ›´ã—ã¦ã‚‚ã€ä¿®æ­£æ¼ã‚ŒãŒå­˜åœ¨ã—ãŸå ´åˆã¯æ°—ä»˜ã‘ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

## çµ‚ã‚ã‚Šã«

CIã®å®Ÿè£…ã¯å¾Œã«å›ã•ã‚ŒãŒã¡ã§ã™ãŒã€ãªã‚‹ã¹ãæ—©ãå®Ÿè£…ã™ã‚‹ã“ã¨ã§é–‹ç™ºã‚¹ãƒ”ãƒ¼ãƒ‰ã‚„åŠ¹ç‡æ€§ãŒå¤§ããå¤‰ã‚ã£ã¦ãã‚‹ã¨æ€ã„ã¾ã™ã€‚
ã‚ã¨ã‹ã‚‰è¦‹è¿”ã—ãŸã‚‰ã€CIã‚’æ—©ã„æ®µéšã§è±Šå¯Œã«ã—ãŸãŠã‹ã’ã§hotfixã®æ•°ãŒæ¿€æ¸›ã—ãŸãªã‚“ã¦ã“ã¨ã‚‚ã‚ˆãèãã¾ã™ã€‚
ãœã²ã”å‚è€ƒã«ãªã‚Œã°å¹¸ã„ã§ã™ã€‚
